# tinyllama-on-intel-npu
# 🦙 TinyLlama on Intel NPU with OpenVINO

I made **TinyLlama LLM** run on my **Intel Ultra Series 1 NPU** using **OpenVINO**.  
This project shows how to run a lightweight chatbot locally on your laptop’s NPU 🚀.

---

## 📂 Project Features
- Runs **TinyLlama** model on Intel NPU
- Uses **OpenVINO** for optimized inference
- INT4 quantization for faster performance

---

## ⚡ Requirements
Install these before running:
```bash
pip install -r requirements.txt

---

## 🚀 Next Steps
- Add your **conversion commands** (like above) to the README.  
- Mention **why INT4 matters** (smaller, faster, NPU-friendly).  
- Push updated README so GitHub/LinkedIn visitors see you did more than just “run the model.”  

---

