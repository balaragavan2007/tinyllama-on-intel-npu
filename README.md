# tinyllama-on-intel-npu
# ğŸ¦™ TinyLlama on Intel NPU with OpenVINO

I made **TinyLlama LLM** run on my **Intel Ultra Series 1 NPU** using **OpenVINO**.  
This project shows how to run a lightweight chatbot locally on your laptopâ€™s NPU ğŸš€.

---

## ğŸ“‚ Project Features
- Runs **TinyLlama** model on Intel NPU
- Uses **OpenVINO** for optimized inference
- INT4 quantization for faster performance

---

## âš¡ Requirements
Install these before running:
```bash
pip install -r requirements.txt

---

## ğŸš€ Next Steps
- Add your **conversion commands** (like above) to the README.  
- Mention **why INT4 matters** (smaller, faster, NPU-friendly).  
- Push updated README so GitHub/LinkedIn visitors see you did more than just â€œrun the model.â€  

---

