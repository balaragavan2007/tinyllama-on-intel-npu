# tinyllama-on-intel-npu
# ðŸ¦™ TinyLlama on Intel NPU with OpenVINO

I made **TinyLlama LLM** run on my **Intel Ultra Series 1 NPU** using **OpenVINO**.  
This project shows how to run a lightweight chatbot locally on your laptopâ€™s NPU ðŸš€.

---

## ðŸ“‚ Project Features
- Runs **TinyLlama** model on Intel NPU
- Uses **OpenVINO** for optimized inference
- Chatbot interface (like ChatGPT) in Python
- INT4 quantization for faster performance

---

## âš¡ Requirements
Install these before running:
```bash
pip install -r requirements.txt

